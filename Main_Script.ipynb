{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import gamma\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.cp_tensor import cp_mode_dot\n",
    "import tensorly.tenalg as tnl\n",
    "from tensorly.tenalg.core_tenalg import tensor_dot, batched_tensor_dot, outer, inner\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from pca import PCA\n",
    "\n",
    "# Import TensorLy\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import kronecker\n",
    "from tensorly import norm\n",
    "from tensorly.decomposition import symmetric_parafac_power_iteration as sym_parafac\n",
    "from tensorly.tenalg.core_tenalg.tensor_product import batched_tensor_dot\n",
    "from tensorly.testing import assert_array_equal, assert_array_almost_equal\n",
    "\n",
    "from tensorly.contrib.sparse.cp_tensor import cp_to_tensor\n",
    "\n",
    "from tlda_final import TLDA\n",
    "import cumulant_gradient\n",
    "import tensor_lda_util as tl_util\n",
    "## Break down into steps, then re-engineer.\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [porter.stem(self.wnl.lemmatize(t,get_wordnet_pos(t))) for t in word_tokenize(articles)]\n",
    "    \n",
    "class StemTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.porter = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.porter.stem(t) for t in word_tokenize(articles)]\n",
    "        \n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data and convert to tensor\n",
    "n_samples = 300000\n",
    "df         = pd.read_csv(\"../Data/TwitterSpeech.csv\")\n",
    "df_p       = pd.read_csv(\"../Data/paradigm.csv\")\n",
    "print(df.head())\n",
    "\n",
    "stop_words = (stopwords.words('english'))\n",
    "added_words = [\"amendment\",\"family\",\"get\",\"adam\",\"hear\",\"feder\",\"de\",\"la\",\"los\",\"democrat\",\"republican\",\n",
    "               'el', 'para', 'en', 'que',\"lo\",\n",
    "               \"amend\",\"back\",\"protect\",\"commun\",\"service\",\"work\",\"around\",\"alway\",\"november\",\"august\",\"january\",\n",
    "               \"happen\",\"ive\",\"hall\",\"nation\",\"work\",\"service\",\"this\",\"discuss\",\"community\",\"learn\",\"congressional\",\"amendment\",\"speaker\",\"say\",\n",
    "               \"said\",\"talk\",\"congrats\",\"pelosi\",\"gop\",\"congratulations\",\"are\",\"as\",\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "               \"you\", \"your\", \"yours\",\"he\",\"her\",\"him\",\"she\",\"hers\",\"that\",\"be\",\"with\",\"their\",\"they're\",\"is\",\"was\",\"been\",\"not\",\"they\",\"it\",\"have\",\n",
    "               \"will\",\"has\",\"by\",\"for\",\"madam\",\"Speaker\",\"Mister\",\"Gentleman\",\"Gentlewoman\",\"lady\",\"voinovich\",\"kayla\",\"111th\",\"115th\",\"114th\",\"rodgers\",      \n",
    "               \"clerk\" ,    \"honor\" ,   \"address\"   ,     \n",
    "               \"house\" , \"start\"   ,\"amend\",\"bipartisan\",\"bill\",   \"114th\"    ,   \"congress\"  ,     \n",
    "               \"one\",   \"thing\"    ,\"bring\",\"put\", \"north\",\"give\",\"keep\",\"pa\",\"even\",\"texa\",\"year\",\"join\",\"well\",\n",
    "               \"call\",  \"learned\"    ,   \"legislator\",\"things\" ,\"things\",\"can't\",\"can\",\"cant\",\"will\",\"go\",\"going\",\"let\",\n",
    "               \"lets\",\"let's\",\"say\",\"says\",\"know\",\"talk\",\"talked\",\"talks\",\"lady\",\"honorable\",\"dont\",\"think\",\"said\",\"something\",\n",
    "               \"something\",\"wont\",\"people\",\"make\",\"want\",\"went\",\"goes\",\"congressmen\",\"people\",\"person\",\"like\",\"come\",\"from\",\n",
    "               \"need\",\"us\"]\n",
    "\n",
    "stop_words= list(np.append(stop_words,added_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-process Data\n",
    "print(int(0.002*n_samples))\n",
    "countvec = CountVectorizer(tokenizer=StemTokenizer(),\n",
    "                                strip_accents = 'unicode', # works \n",
    "                                stop_words = stop_words, # works\n",
    "                                lowercase = True, # works\n",
    "                                ngram_range = (1,2),\n",
    "                                max_df = 0.4, # works\n",
    "                                min_df = int(0.002*n_samples))\n",
    "\n",
    "dtm           = countvec.fit_transform(df.tweet[ df.year>=2019][:n_samples])\n",
    "\n",
    "print(dtm.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # find to sentiments\n",
    "    sum_words = dtm.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     countvec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    print(words_freq[:100])\n",
    "    \n",
    "    top_sents = [ i[0] for i in  words_freq[:1000] if i[0] in df_p[\"Token\"].unique() ]\n",
    "    print(top_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_pos = np.float_(dtm.toarray())\n",
    "dtm_neu = np.float_(dtm.toarray())\n",
    "dtm_neg = np.float_(dtm.toarray())\n",
    "for i,v in countvec.vocabulary_.items():\n",
    "\n",
    "        if i in df_p[\"Token\"].unique() and i in top_sents[:100]:\n",
    "            print(i)\n",
    "            print(df_p.Positive[df_p.Token==i])\n",
    "            dtm_pos[:,v] *= df_p.Positive[df_p.Token==i].unique() \n",
    "            dtm_neg[:,v] *= df_p.Negative[df_p.Token==i].unique() \n",
    "            dtm_neu[:,v] *= df_p.Neutral[df_p.Token==i].unique()\n",
    "        else:\n",
    "            dtm_pos[:,v] *= 1/3\n",
    "            dtm_neg[:,v] *= 1/3\n",
    "            dtm_neu[:,v] *= 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sent=scipy.sparse.csr_matrix(np.concatenate((dtm_pos,dtm_neg,dtm_neu),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtm_pos,dtm_neg,dtm_neu\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a       = tl.tensor(dtm_sent.toarray(),dtype=np.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtm_sent\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M1      = tl.mean(a, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_cent = scipy.sparse.csr_matrix(a - M1,dtype=np.float16) #center the data using the first moment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        start = datetime.now()\n",
    "        print(\"now =\", start)\n",
    "    \n",
    "    \n",
    "        batch_size    = np.int(n_samples/20)\n",
    "        verbose       = True \n",
    "        n_topic=  20\n",
    "        \n",
    "        beta_0=0.003\n",
    "        \n",
    "        pca = PCA(n_topic, beta_0, 30000)\n",
    "        pca.fit(x_cent) # fits PCA to  data, gives W\n",
    "        x_whit = pca.transform(x_cent) # produces a whitened words counts <W,x> for centered data x\n",
    "        now = datetime.now()\n",
    "        print(\"now =\", now)\n",
    "        pca_time = now- start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "print(pca_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import tlda_final\n",
    "reload(tlda_final)\n",
    "from tlda_final import TLDA\n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "learning_rate = 0.01 \n",
    "batch_size =15000\n",
    "t = TLDA(n_topic,n_senti=3, alpha_0= beta_0, n_iter_train=1000, n_iter_test=150, batch_size=batch_size,\n",
    "         learning_rate=learning_rate)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "t.fit(x_whit,verbose=True) # fit whitened wordcounts to get decomposition of M3 through SGD\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "\n",
    "\n",
    "t.factors_ = pca.reverse_transform(t.factors_)  # unwhiten the eigenvectors to get unscaled word-level factors\n",
    "\n",
    "''' \n",
    "Recover alpha_hat from the eigenvalues of M3\n",
    "'''  \n",
    "\n",
    "eig_vals = [np.linalg.norm(k,3) for k in t.factors_ ]\n",
    "# normalize beta\n",
    "alpha      = np.power(eig_vals, -2)\n",
    "print(alpha.shape)\n",
    "alpha_norm = (alpha / alpha.sum()) * beta_0\n",
    "t.alpha_   = alpha_norm\n",
    "        \n",
    "print(alpha_norm)\n",
    "\n",
    "t.predict(x_whit,w_mat=True,doc_predict=False)  # normalize the factors \n",
    "\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factors= t.factors_\n",
    "print((factors.shape))\n",
    "factors_reshape = np.concatenate((factors[:,0:(factors.shape[1]//3)],\n",
    "                                  factors[:,(factors.shape[1]//3):(2*factors.shape[1]//3)],\n",
    "                                  factors[:,(2*factors.shape[1]//3):(factors.shape[1])]),axis=0)\n",
    "factors_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.factors_ = factors_reshape\n",
    "#t.factors_ = factors\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "\n",
    "for k in range(n_topic*3):\n",
    "    if k ==0:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   =t.factors_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_JST = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])\n",
    "        print([i for i,v in countvec.vocabulary_.items() if v in t_n_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "now = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "print(\"now =\", now)\n",
    "print(t.factors_.shape)\n",
    "a_word       = tl.tensor(dtm.toarray(),dtype=tl.float32)\n",
    "\n",
    "doc_topic_dist, topic_word_dist = t.predict(a_word,w_mat=False,doc_predict=True)\n",
    "now = datetime.now()\n",
    " \n",
    "print(\"now =\", now)\n",
    "end = datetime.now()\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_sparse = scipy.sparse.csc_matrix(dtm.toarray(),dtype=np.float16)\n",
    "dtm_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_gensim = datetime.now()\n",
    "lda = LatentDirichletAllocation(n_components=84,n_jobs=-1,\n",
    "                                learning_method=\"online\",verbose=1,max_iter=1000,\n",
    "                                evaluate_every=10,batch_size=15000,max_doc_update_iter=150,perp_tol=1e-3)\n",
    "lda.fit(dtm)\n",
    "\n",
    "\n",
    "end_1 = datetime.now()\n",
    "\n",
    "doc_topic_LDA = lda.transform(dtm)\n",
    "end_2 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end-start)\n",
    "print(end_2-start_gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words=20\n",
    "#print(t_n_indices)\n",
    "\n",
    "for k in range(n_topic*3):\n",
    "    if k ==0:\n",
    "        t_n_indices   =lda.components_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_LDA = [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]\n",
    "    else:\n",
    "        t_n_indices   = lda.components_[k,:].argsort()[:-n_top_words - 1:-1]\n",
    "        top_words_LDA = np.vstack([top_words_JST, [i for i,v in countvec.vocabulary_.items() if v in t_n_indices]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_words_LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topic_LDA).to_csv(\"../Data/theta_LDA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(doc_topic_dist).to_csv(\"../Data/theta_JST_Tensor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_words_LDA).to_csv(\"../Data/theta_LDA_TopWords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top_words_JST).to_csv(\"../Data/theta_JST_TopWords.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
